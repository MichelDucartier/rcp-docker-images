flash-attn<=2.4.2 # Check https://github.com/Dao-AILab/flash-attention/issues/867
einops
monai
open_clip_torch
deepspeed
transformers==4.45.0
